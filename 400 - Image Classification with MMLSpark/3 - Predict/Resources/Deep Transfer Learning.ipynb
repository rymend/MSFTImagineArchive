{"nbformat_minor": 2, "cells": [{"source": "# Painting Image Recognition with Deep Transfer Learning\n\nThis hands-on tutorial shows how to use [Transfer Learning](https://en.wikipedia.org/wiki/Inductive_transfer) to take an existing trained model and adapt it to the painting data.  \n\nThis notebook lab adapts and updates the tutorial from the Microsoft tutorial at https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_301_Image_Recognition_with_Deep_Transfer_Learning.ipynb.\n\n## Problem\nYou acquired a set of painting images which need to be classified.\n\n\nHowever, the number of images is far less than what is needed to train a state-of-the-art classifier such as a [Residual Network](https://github.com/KaimingHe/deep-residual-networks). You have a rich annotated data set of images of natural scene images such as shown below (courtesy [t-SNE visualization site](http://cs.stanford.edu/people/karpathy/cnnembed/)).\n\n![](http://www.cntk.ai/jup/cntk301_imagenet.jpg)\n\nThis tutorial introduces deep transfer learning as a means to leverage multiple data sources to overcome data scarcity problem.\n\n### Why Transfer Learning?\n\nAs stated above, Transfer Learning is a useful technique when, for instance, you know you need to classify incoming images into different categories, but you do not have enough data to train a Deep Neural Network (DNN) from scratch. Training DNNs takes a lot of data, all of it labeled, and often you will not have that kind of data on hand. If your problem is similar to one for which a network has already been trained, though, you can use Transfer Learning to modify that network to your problem with a fraction of the labeled images (we are talking tens instead of thousands). \n\n### What is Transfer Learning?\n\nWith Transfer Learning, we use an existing trained model and adapt it to our own problem. We are essentially building upon the features and concepts that were learned during the training of the base model. With a Convolutional DNN (ResNet_18 in this case), we are using the features learned from ImageNet data and _cutting off_ the final classification layer, replacing it with a new dense layer that will predict the class labels of our new domain. \n\nThe input to the old and the new prediction layer is the same, we simply reuse the trained features. Then we train this modified network, either only the new weights of the new prediction layer or all weights of the entire network.\n\nThis can be used, for instance, when we have a small set of images that are in a similar domain to an existing trained model. Training a Deep Neural Network from scratch requires tens of thousands of images, but training one that has already learned features in the domain you are adapting it to requires far fewer. \n\n\nIn our case, this means adapting a network trained on ImageNet images (dogs, cats, birds, etc.) to paintings. However, Transfer Learning has also been successfully used to adapt existing neural models for translation, speech synthesis, and many other domains - it is a convenient way to bootstrap your learning process.\n\n**Importing CNTK and other useful libraries**\n\nMicrosoft's Cognitive Toolkit comes in Python form as `cntk`, and contains many useful submodules for IO, defining layers, training models, and interrogating trained models. We will need many of these for Transfer Learning, as well as some other common libraries for downloading files, unpacking/unzipping them, working with the file system, and loading matrices.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import glob\nimport os\nimport sys\nimport time\n\n# https://pypi.python.org/pypi/Pillow\nfrom PIL import Image\nimport numpy as np\n\n# Load the right urlretrieve based on python version\ntry: \n    from urllib.request import urlretrieve \nexcept ImportError: \n    from urllib import urlretrieve\n    \nfrom pathlib import Path\n\n# Useful for being able to dump images into the Notebook\nimport IPython.display as D\n\n# Import CNTK and helpers\nimport cntk as C", "outputs": [], "metadata": {"collapsed": false}}, {"source": "There are two run modes:\n- *Fast mode*: `isFast` is set to `True`. This is the default mode for the notebooks, which means we train for fewer iterations or train / test on limited data. This ensures functional correctness of the notebook though the models produced are far from what a completed training would produce.\n\n- *Slow mode*: We recommend the user to set this flag to `False` once the user has gained familiarity with the notebook content and wants to gain insight from running the notebooks for a longer period with different parameters for training. \n\nFor *Fast mode* we train the model for 5 epochs and results have low accuracy but is good enough for development. The model yields good accuracy after 10-20 epochs.\n\nFeel free to adjust the learning_params below and observe the results. You can tweak the **max_epochs** to train for longer, **mb_size** to adjust the size of each minibatch, or **lr_per_mb** to play with the speed of convergence (learning rate). \n\nNote that if you've already trained the model, you will want to set force_retraining to **True** to force the Notebook to re-train your model with the new parameters. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "isFast = True\n#isFast = False\n\n# Set hyperparameters for the transfer learning\nforce_retraining = True\n\nmax_training_epochs = 7 if isFast else 20\n\nlearning_params = {\n    'max_epochs': max_training_epochs,\n    'mb_size': 50,\n    'lr_per_mb': [0.2]*10 + [0.1],\n    'momentum_per_mb': 0.9,\n    'l2_reg_weight': 0.0005,\n    'freeze_weights': True\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Data Access\n\nThis code sets the directories in the local file system.  The commands have a function for ensuring a directory exists, and also that files would be downloaded from a URI unless they already exist in the target location.  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# By default, we store data in the Image directory\ndata_root = os.path.abspath(os.path.join('..', 'Image'))\n    \ndatasets_path = os.path.join(data_root, 'DataSets')\noutput_path = os.path.abspath(os.path.join('.', 'temp', 'Output'))\n\ndef ensure_exists(path):\n    if not os.path.exists(path):\n        print('Making Directory: ', path)\n        os.makedirs(path)\n        \ndef download_unless_exists(url, filename, max_retries=3):\n    '''Retrieve the file unless it already exists, with retry. Throws if all retries fail.'''\n    if os.path.exists(filename):\n        print('Reusing locally cached: ', filename)\n    else:\n        print('Starting retrieval of {} to {}'.format(url, filename))\n        retry_cnt = 0\n        while True:\n            try:\n                urlretrieve(url, filename)\n                print('File retrieval completed.')\n                return\n            except:\n                retry_cnt += 1\n                if retry_cnt == max_retries:\n                    print('Exceeded maximum retry count, aborting.')\n                    raise\n                print('Failed to retrieve, retrying.')\n                time.sleep(np.random.randint(1,10))        \n\nensure_exists(datasets_path)\nensure_exists(output_path)\nprint ('datasets_path: ', Path(datasets_path).resolve())\nprint ('output_path: ', Path(output_path).resolve())", "outputs": [], "metadata": {"collapsed": false}}, {"source": "In the next section of code, we are transferring the files from the location in Azure Blob Storage to the filesystem.  Note that you will need to add the name of the CONTAINER that is specific for your application.  If your blob storage is attached to the Spark Cluster, your credentials are saved, and therefore you may access files with the **wasb** syntax.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "azure_blob_root = 'wasbs://images@marktabblob.blob.core.windows.net/'\npaintings_stem = 'Image/DataSets/Paintings/'\ntraining_stem = paintings_stem + 'Train/'\ntesting_stem = paintings_stem + 'Test/'\n\n# Transferring the image files from the Azure Blob Storage uri into the Linux file system\ndef transfer_paintings_dataset(dataset_root = os.path.join(datasets_path, 'Paintings')):\n    training_dir = os.path.join(dataset_root, 'Train')\n    testing_dir = os.path.join(dataset_root, 'Test')\n    \n    # Make Directories if needed\n    ensure_exists(dataset_root)\n    ensure_exists(training_dir)\n    ensure_exists(testing_dir)    \n    \n    transferfilelocation = azure_blob_root + paintings_stem + 'transferimages.txt'\n    transferfile = spark.sparkContext.textFile(transferfilelocation)\n    for row in transferfile.collect():\n        uri = str(row.split(' ', 1)[0])\n        dirname = os.path.join(dataset_root, str(row.split(' ', 2)[1]))\n        filename = os.path.join(dataset_root, str(row.split(' ', 2)[1]), str(row.split(' ', 2)[2]))\n        \n        # Creates Linux directory if needed\n        ensure_exists(dirname)\n        download_unless_exists(uri, filename) \n    print (len(transferfile.collect()),' Images Transferred')        \n    return {\n        'training_folder': training_dir,\n        'testing_folder': testing_dir\n    }\n\nprint('Transferring paintings from Azure Blob Storage to Linux file system: this transfer might take a while...')\npaintings_data = transfer_paintings_dataset()\nprint('All data now available to the notebook!')\n\npaintings_data", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Pre-Trained Model (ResNet)\n\nFor this task, we have chosen ResNet_18 as our trained model and  will it as the base model. This model will be adapted using Transfer Learning for classification of flowers and animals. This model is a [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) built using [Residual Network](https://github.com/KaimingHe/deep-residual-networks) techniques. Convolutional Neural Networks build up layers of convolutions, transforming an input image and distilling it down until they start recognizing composite features, with deeper layers of convolutions recognizing complex patterns are made possible. The author of Keras has a [fantastic post](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html) where he describes how Convolutional Networks \"see the world\" which gives a much more detailed explanation.\n\nResidual Deep Learning is a technique that originated in Microsoft Research and involves \"passing through\" the main signal of the input data, so that the network winds up \"learning\" on just the residual portions that differ between layers. This has proven, in practice, to allow the training of much deeper networks by avoiding issues that plague gradient descent on larger networks. These cells bypass convolution layers and then come back in later before ReLU (see below), but some have argued that even deeper networks can be built by avoiding even more nonlinearities in the bypass channel. This is an area of hot research right now, and one of the most exciting parts of Transfer Learning is that you get to benefit from all of the improvements by just integrating new trained models.\n\n![](https://adeshpande3.github.io/assets/ResNet.png)\n\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "def download_model(model_root = os.path.join(data_root, 'PretrainedModels')):\n    ensure_exists(model_root)\n    resnet18_model_uri = 'https://www.cntk.ai/Models/ResNet/ResNet_18.model'\n    resnet18_model_local = os.path.join(model_root, 'ResNet_18.model')\n    download_unless_exists(resnet18_model_uri, resnet18_model_local)\n    print('Downloaded model to: ', Path(resnet18_model_local).resolve())\n    return resnet18_model_local\n\nprint('Downloading pre-trained model. Note: this process might take a while...')\n# Download pretrained model to a Linux directory attached to this application instance\nbase_model_file = download_model()\nprint('Downloading pre-trained model complete!')", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Inspecting pre-trained model\n\nWe print out all of the layers in ResNet_18 to show you how you can interrogate a model - to use a different model than ResNet_18 you would just need to discover the appropriate last hidden layer and feature layer to use. CNTK provides a convenient `get_node_outputs` method under `cntk.graph` to allow you to dump all of the model details. We can recognize the final hidden layer as the one before we start computing the final classification into the 1000 ImageNet classes (so in this case, `z.x`).", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# define base model location and characteristics\nbase_model = {\n    'model_file': base_model_file,\n    'feature_node_name': 'features',\n    'last_hidden_node_name': 'z.x',\n    # Channel Depth x Height x Width\n    'image_dims': (3, 224, 224)\n}\n\n# Print out all layers in the model\nprint('Loading {} and printing all layers:'.format(base_model['model_file']))\nnode_outputs = C.logging.get_node_outputs(C.load_model(base_model['model_file']))\nfor l in node_outputs: print(\"  {0} {1}\".format(l.name, l.shape))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Training the Transfer Learning Model\n\nIn the code below, we load up the pre-trained ResNet_18 model and clone it, while stripping off the final `features` layer. We clone the model so that we can re-use the same trained model multiple times, trained for different things - it is not strictly necessary if you are just training it for a single task, but this is why we would not use `CloneMethod.share`, we want to learn new parameters. If `freeze_weights` is true, we will freeze weights on all layers we clone and only learn weights on the final new features layer. This can often be useful if you are cloning higher up the tree (e.g., cloning after the first convolutional layer to just get basic image features).\n\nWe find the final hidden layer (`z.x`) using `find_by_name`, clone it and all of its predecessors, then attach a new `Dense` layer for classification.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import cntk.io.transforms as xforms\nensure_exists(output_path)\nnp.random.seed(123)\n\n# Creates a minibatch source for training or testing\ndef create_mb_source(map_file, image_dims, num_classes, randomize=True):\n    transforms = [xforms.scale(width=image_dims[2], height=image_dims[1], channels=image_dims[0], interpolations='linear')]\n    return C.io.MinibatchSource(C.io.ImageDeserializer(map_file, C.io.StreamDefs(\n            features=C.io.StreamDef(field='image', transforms=transforms),\n            labels=C.io.StreamDef(field='label', shape=num_classes))),\n            randomize=randomize)\n\n# Creates the network model for transfer learning\ndef create_model(model_details, num_classes, input_features, new_prediction_node_name='prediction', freeze=False):\n    # Load the pretrained classification net and find nodes\n    base_model = C.load_model(model_details['model_file'])\n    feature_node = C.logging.find_by_name(base_model, model_details['feature_node_name'])\n    last_node = C.logging.find_by_name(base_model, model_details['last_hidden_node_name'])\n\n    # Clone the desired layers with fixed weights\n    cloned_layers = C.combine([last_node.owner]).clone(\n        C.CloneMethod.freeze if freeze else C.CloneMethod.clone,\n        {feature_node: C.placeholder(name='features')})\n\n    # Add new dense layer for class prediction\n    feat_norm = input_features - C.Constant(114)\n    cloned_out = cloned_layers(feat_norm)\n    z = C.layers.Dense(num_classes, activation=None, name=new_prediction_node_name) (cloned_out)\n\n    return z", "outputs": [], "metadata": {"collapsed": true}}, {"source": "We will now train the model just like any other CNTK model training - instantiating an input source (in this case a `MinibatchSource` from our image data), defining the loss function, and training for a number of epochs. Since we are training a multi-class classifier network, the final layer is a cross-entropy Softmax, and the error function is classification error - both conveniently provided by utility functions in `cntk.ops`.\n\nWhen training a pre-trained model, we are adapting the existing weights to suit our domain. Since the weights are likely already close to correct (especially for earlier layers that find more primitive features), fewer examples and fewer epochs are typically required to get good performance.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Trains a transfer learning model\ndef train_model(model_details, num_classes, train_map_file,\n                learning_params, max_images=-1):\n    num_epochs = learning_params['max_epochs']\n    epoch_size = sum(1 for line in open(train_map_file))\n    if max_images > 0:\n        epoch_size = min(epoch_size, max_images)\n    minibatch_size = learning_params['mb_size']\n    \n    # Create the minibatch source and input variables\n    minibatch_source = create_mb_source(train_map_file, model_details['image_dims'], num_classes)\n    image_input = C.input_variable(model_details['image_dims'])\n    label_input = C.input_variable(num_classes)\n\n    # Define mapping from reader streams to network inputs\n    input_map = {\n        image_input: minibatch_source['features'],\n        label_input: minibatch_source['labels']\n    }\n\n    # Instantiate the transfer learning model and loss function\n    tl_model = create_model(model_details, num_classes, image_input, freeze=learning_params['freeze_weights'])\n    ce = C.cross_entropy_with_softmax(tl_model, label_input)\n    pe = C.classification_error(tl_model, label_input)\n\n    # Instantiate the trainer object\n    lr_schedule = C.learning_parameter_schedule(learning_params['lr_per_mb'])\n    mm_schedule = C.momentum_schedule(learning_params['momentum_per_mb'])\n    learner = C.momentum_sgd(tl_model.parameters, lr_schedule, mm_schedule, \n                           l2_regularization_weight=learning_params['l2_reg_weight'])\n    trainer = C.Trainer(tl_model, (ce, pe), learner)\n\n    # Get minibatches of images and perform model training\n    print(\"Training transfer learning model for {0} epochs (epoch_size = {1}).\".format(num_epochs, epoch_size))\n    C.logging.log_number_of_parameters(tl_model)\n    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=num_epochs)\n    for epoch in range(num_epochs):       # loop over epochs\n        sample_count = 0\n        while sample_count < epoch_size:  # loop over minibatches in the epoch\n            data = minibatch_source.next_minibatch(min(minibatch_size, epoch_size - sample_count), input_map=input_map)\n            trainer.train_minibatch(data)                                    # update model with it\n            sample_count += trainer.previous_minibatch_sample_count          # count samples processed so far\n            progress_printer.update_with_trainer(trainer, with_metric=True)  # log progress\n            if sample_count % (100 * minibatch_size) == 0:\n                print (\"Processed {0} samples\".format(sample_count))\n\n        progress_printer.epoch_summary(with_metric=True)\n\n    return tl_model", "outputs": [], "metadata": {"collapsed": true}}, {"source": "When we evaluate the trained model on an image, we have to massage that image into the expected format. In our case we use `Image` to load the image from its path, resize it to the size expected by our model, reverse the color channels (RGB to BGR), and convert to a contiguous array along height, width, and color channels. This corresponds to the 224x224x3 flattened array on which our model was trained.\n\nThe model with which we are doing the evaluation has not had the Softmax and Error layers added, so is complete up to the final feature layer. To evaluate the image with the model, we send the input data to the `model.eval` method, `softmax` over the results to produce probabilities, and use Numpy's `argmax` method to determine the predicted class. We may then compare that against the true labels to get the overall model accuracy.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Evaluates a single image using the re-trained model\ndef eval_single_image(loaded_model, image_path, image_dims):\n    # load and format image (resize, RGB -> BGR, CHW -> HWC)\n    try:\n        img = Image.open(image_path).convert(\"RGBA\")\n        \n        if image_path.endswith(\"png\"):\n            temp = Image.new(\"RGB\", img.size, (255, 255, 255))\n            temp.paste(img, img)\n            img = temp\n        resized = img.resize((image_dims[2], image_dims[1]), Image.ANTIALIAS)\n        bgr_image = np.asarray(resized, dtype=np.float32)[..., [2, 1, 0]]\n        hwc_format = np.ascontiguousarray(np.rollaxis(bgr_image, 2))\n\n        # compute model output\n        arguments = {loaded_model.arguments[0]: [hwc_format]}\n        output = loaded_model.eval(arguments)\n\n        # return softmax probabilities\n        sm = C.softmax(output[0])\n        return sm.eval()\n    except FileNotFoundError:\n        print(\"Could not open (skipping file): \", image_path)\n        return ['None']\n        \n\n\n# Evaluates an image set using the provided model\ndef eval_test_images(loaded_model, output_file, test_map_file, image_dims, max_images=-1, column_offset=0):\n    num_images = sum(1 for line in open(test_map_file))\n    if max_images > 0:\n        num_images = min(num_images, max_images)\n    if isFast:\n        num_images = min(num_images, 300) #We will run through fewer images for test run\n        \n    print(\"Evaluating model output node '{0}' for {1} images.\".format('prediction', num_images))\n\n    pred_count = 0\n    correct_count = 0\n    np.seterr(over='raise')\n    with open(output_file, 'wb') as results_file:\n        with open(test_map_file, \"r\") as input_file:\n            for line in input_file:\n                tokens = line.rstrip().split('\\t')\n                img_file = tokens[0 + column_offset]\n                probs = eval_single_image(loaded_model, img_file, image_dims)\n                \n                if probs[0]=='None':\n                    print(\"Eval not possible: \", img_file)\n                    continue\n\n                pred_count += 1\n                true_label = int(tokens[1 + column_offset])\n                predicted_label = np.argmax(probs)\n                if predicted_label == true_label:\n                    correct_count += 1\n\n                #np.savetxt(results_file, probs[np.newaxis], fmt=\"%.3f\")\n                if pred_count % 100 == 0:\n                    print(\"Processed {0} samples ({1:.2%} correct)\".format(pred_count, \n                                                                           (float(correct_count) / pred_count)))\n                if pred_count >= num_images:\n                    break\n    print (\"{0} of {1} prediction were correct\".format(correct_count, pred_count))\n    return correct_count, pred_count, (float(correct_count) / pred_count)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Finally, with all of these helper functions in place we can train the model and evaluate it on our paintings dataset.\n\nYou should see the model train and evaluate, with a estimated accuracy at over 80% (results may vary if the data are being acquired dynamically). At this point you could choose to train longer, or consider taking a look at the confusion matrix to determine if certain flowers are mis-predicted at a greater rate. You could also easily swap out to a different model and see if that performs better, or potentially learn from an earlier point in the model architecture.  The hyperparameter settings appear toward the top of this notebook.", "cell_type": "markdown", "metadata": {}}, {"source": "Azure Blob storage has no real directories:  the container simply stores key-value pairs.  The slash character gives an appearance of directories, but these slashes are simply part of the key name.  Earlier the image files were transferred into the local filesystem, allowing for the next step:  building a map file based on all images which appear in the directory.  Creating a map file dynamically matches the real-world situation where files on a list (to be downloaded) may not actually be available or may not actually finish loading into the intended location.\n\nThe images are stored with `Train` and `Test` designators with the nested designator giving the class name (i.e. `MonetPaintings` and `VanGoghPaintings` folders). The code below creates the mapping file based on what CNTK expects:  a row for each file with the name, then followed by a class designation integer. It's important that spaces are NOT in the filename or its directory path for the purpose of this code.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Set python version variable \npython_version = sys.version_info.major\n\ndef create_map_file_from_folder(root_folder, class_mapping, include_unknown=False, valid_extensions=['.jpg', '.jpeg', '.png']):\n    map_file_name = os.path.join(root_folder, \"map.txt\")\n    \n    map_file = None\n\n    if python_version == 3: \n        map_file = open(map_file_name , 'w', encoding='utf-8')\n    else:\n        map_file = open(map_file_name , 'w')\n\n    print(len(class_mapping), ' Total Classes')\n    for class_id in range(0, len(class_mapping)):\n        folder = os.path.abspath(os.path.join(root_folder, class_mapping[class_id]))\n        print(folder)\n        if os.path.exists(folder):\n            print ('path exists')\n            for entry in os.listdir(folder):\n                filename = os.path.abspath(os.path.join(folder, entry))\n                if os.path.isfile(filename) and os.path.splitext(filename)[1].lower() in valid_extensions:\n                    try:\n                        map_file.write(\"{0}\\t{1}\\n\".format(filename, class_id))\n                    except UnicodeEncodeError:\n                        continue\n\n    if include_unknown:\n        for entry in os.listdir(root_folder):\n            filename = os.path.abspath(os.path.join(root_folder, entry))\n            if os.path.isfile(filename) and os.path.splitext(filename)[1].lower() in valid_extensions:\n                try:\n                    map_file.write(\"{0}\\t-1\\n\".format(filename))\n                except UnicodeEncodeError:\n                    continue\n                    \n    map_file.close()  \n    \n    return map_file_name\n  \n\ndef create_class_mapping_from_folder(root_folder):\n    print(root_folder)\n    print(Path(root_folder).resolve())\n    classes = []\n    for _, directories, _ in os.walk(root_folder):\n        for directory in directories:\n            classes.append(directory)\n            print ('Appending from ', directory)\n    return np.asarray(classes)\n\npaintings_data['class_mapping'] = create_class_mapping_from_folder(paintings_data['training_folder'])\npaintings_data['training_map'] = create_map_file_from_folder(paintings_data['training_folder'], paintings_data['class_mapping'])\n\n# Allows for adding additional images which were never seen\npaintings_data['testing_map'] = create_map_file_from_folder(paintings_data['testing_folder'], paintings_data['class_mapping'], \n                                                          include_unknown=True)\npaintings_data", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We may now train our model on our small domain and evaluate the results.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "paintings_model = {\n    'model_file': os.path.join(output_path, 'PaintingsTransferLearning.model'),\n    'results_file': os.path.join(output_path, 'PaintingsPredictions.txt'),\n    'num_classes': len(paintings_data['class_mapping'])\n}\n\nif os.path.exists(paintings_model['model_file']) and not force_retraining:\n    print(\"Loading existing model from %s\" % paintings_model['model_file'])\n    trained_model = C.load_model(paintings_model['model_file'])\nelse:\n    trained_model = train_model(base_model, \n                                paintings_model['num_classes'], paintings_data['training_map'],\n                                learning_params)\n    trained_model.save(paintings_model['model_file'])\n    print(\"Stored trained model at %s\" % paintings_model['model_file'])    ", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now that the model is trained for paintings data, let's evaluate the images.", "cell_type": "markdown", "metadata": {}}, {"source": "### Evaluate\n\nWe may now evaluate the newly learnt paintings classifier by calculating from a pre-trained ResNet model.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Evaluate the test set\npredict_correct, predict_total, predict_accuracy = \\\n   eval_test_images(trained_model, paintings_model['results_file'], paintings_data['testing_map'], base_model['image_dims'])\nprint(\"Done. Wrote output to %s\" % paintings_model['results_file'])", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "This next code provides a printout of the individual test images, and their probabilities of being in each presented class.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# evaluate test images\nwith open(paintings_data['testing_map'], 'r') as input_file:\n    for line in input_file:\n        tokens = line.rstrip().split('\\t')\n        img_file = tokens[0]\n        true_label = int(tokens[1])\n        probs = eval_single_image(trained_model, img_file, base_model['image_dims'])\n\n        if probs[0]=='None':\n            continue\n        class_probs = np.column_stack((probs, paintings_data['class_mapping'])).tolist()\n        class_probs.sort(key=lambda x: float(x[0]), reverse=True)\n        predictions = ' '.join(['%s:%.3f' % (class_probs[i][1], float(class_probs[i][0])) \\\n                                for i in range(0, paintings_model['num_classes'])])\n        true_class_name = paintings_data['class_mapping'][true_label] if true_label >= 0 else 'unknown'\n        print('Class: %s, predictions: %s, image: %s' % (true_class_name, predictions, img_file))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Finally, we may calculate an overall accuracy metric.  In our testing, we achieved 88% with the five epoch run and 93% with the 20 epoch run, but your results may vary since the images are retrieved dynamically and since your random number assignments are not likely to be identical.  Thus, accuracy results may be greater or smaller.\n\nYou may use cross-validation (say, using five folds) and get an average of the five resulting runs to see a more robust indication of the reliability of this classification technique for this use application.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Test: Accuracy on paintings data\nprint (\"Prediction accuracy: {0:.2%}\".format(float(predict_correct) / predict_total))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Final Thoughts, and Caveats\n\nTransfer Learning has limitations. If you noticed, we re-trained a model that had been trained on ImageNet images. This meant it already _knew_ what \"images\" were, and had a good idea on concepts from low-level (stripes, circles) to high-level (dog's noses, cat's ears). Re-training such a model to detect sheep or wolves makes sense, but re-training it to detect vehicles from aerial imagery would be more difficult. You can still use Transfer Learning in these cases, but you might want to just re-use earlier layers of the model (i.e. the early Convolutional layers that have learned more primitive concepts), and you will likely require much more training data.\n\nAdding a catch-all category can be a good idea, but only if the training data for that category contains images that are again sufficiently similar to the images you expect at scoring time. As in the above example, if we train a classifier with images of sheep and wolf and use it to score an image of a bird, the classifier can still only assign a sheep or wolf label, since it does not know any other categories. If we were to add a catch-all category and add training images of birds to it then the classifier might predict the class correctly for the bird image. However, if we present it, e.g., an image of a car, it faces the same problem as before as it knows only sheep, wolf and bird (which we just happened to call called catch-all). Hence, your training data, even for your catch-all, needs to cover sufficiently those concepts and images that you expect later on at scoring time.", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}, "anaconda-cloud": {}}}